#常用Transformation(即转换，延迟加载)

#通过并行化scala集合创建RDD
val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))
#查看该rdd的分区数量
rdd1.partitions.length

# parallelize 把scala 的集合转化成 spark 的RDD 
val rdd1 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10))
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x,true)

//这里调用的 filter 是 rdd 的方法 不是scala 集合的filter 
val rdd3 = rdd2.filter(_>10)

// sortBy 中的排序函数只会影响集合里面的顺序,不会影响 集合里面的数据

val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x+"",true)
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x.toString,true)



val rdd4 = sc.parallelize(Array("a b c", "d e f", "h i j"))
rdd4.flatMap(_.split(' ')).collect

val rdd5 = sc.parallelize(List(List("a b c", "a b b"),List("e f g", "a f g"), List("h i j", "a a b")))

List("a b c", "a b b") =List("a","b",))

rdd5.flatMap(_.flatMap(_.split(" "))).collect

#union求并集，注意类型要一致
val rdd6 = sc.parallelize(List(5,6,4,7))
val rdd7 = sc.parallelize(List(1,2,3,4))
val rdd8 = rdd6.union(rdd7)

rdd8.distinct.sortBy(x=>x).collect

res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7) 
 
#intersection求交集
val rdd9 = rdd6.intersection(rdd7)

rdd9: Array[Int] = Array(4)  

val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3)))
val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7), ("tom", 2)))

#join(连接) 
val rdd3 = rdd1.join(rdd2)

res16: Array[(String, (Int, Int))] = Array((tom,(1,8)), (tom,(1,2)), (jerry,(2,9)))


# leftOuterJoin 就是左边的必存在,右边的有则用Some 包装,右边没有 就是 None
val rdd3 = rdd1.leftOuterJoin(rdd2)

res18: Array[(String, (Int, Option[Int]))] = Array((tom,(1,Some(8))), (tom,(1,Some(2))), (jerry,(2,Some(9))), (kitty,(3,None)))

# rightOuterJoin 右边必然存在,左边不存在None ,存在就是 Some
val rdd3 = rdd1.rightOuterJoin(rdd2)
Array[(String, (Option[Int], Int))] = Array((tom,(Some(1),8)), (tom,(Some(1),2)), (jerry,(Some(2),9)), (shuke,(None,7)))


#groupByKey  groupByKey 直接拆分数据发送远端进行汇总
val rdd3 = rdd1 union rdd2
rdd3.groupByKey
//(tom,CompactBuffer(1, 8, 2))
rdd3.groupByKey.map(x=>(x._1,x._2.sum))
groupByKey.mapValues(_.sum).collect
Array((tom,CompactBuffer(1, 8, 2)), (jerry,CompactBuffer(9, 2)), (shuke,CompactBuffer(7)), (kitty,CompactBuffer(3)))


#WordCount 示例  reduceByKey 与 groupByKey  reduceByKey 会先在局部进行聚合再发送到远端汇总, groupByKey 直接拆分数据发送远端进行汇总
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).groupByKey.map(t=>(t._1, t._2.sum)).collect

#cogroup 将多个RDD中同一个Key对应的Value组合到一起。
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)))
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2)))

res14: Array[(String, (Iterable[Int], Iterable[Int]))] = 
Array((tom,(CompactBuffer(1, 2),CompactBuffer(1))), 
	  (jerry,(CompactBuffer(3),CompactBuffer(2))), 
	  (shuke,(CompactBuffer(),CompactBuffer(2))), 
	  (kitty,(CompactBuffer(2),CompactBuffer())))


val rdd3 = rdd1.cogroup(rdd2)
val rdd4 = rdd3.map(t=>(t._1, t._2._1.sum + t._2._2.sum))

#cartesian笛卡尔积
val rdd1 = sc.parallelize(List("tom", "jerry"))
val rdd2 = sc.parallelize(List("tom", "kitty", "shuke"))
val rdd3 = rdd1.cartesian(rdd2)

###################################################################################################

#spark action
val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)

#collect
rdd1.collect

#reduce
val r = rdd1.reduce(_+_)

#count
rdd1.count

#top
rdd1.top(2)

#take
rdd1.take(2)

#first(similer to take(1))
rdd1.first

#takeOrdered
rdd1.takeOrdered(3)
